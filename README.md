**Project Description**

This assignment explores the practical use of distributed computing tools for processing large-scale datasets. The work involves installing and configuring Apache Hadoop on a local machine, executing MapReduce programs, managing data using HDFS, and performing advanced analytics with Apache Spark.

The tasks demonstrate how big data frameworks enable scalable storage, parallel computation, and efficient handling of large text collections.

ğŸ¯**Goals of the Assignment**

  ğŸ”¹Set up a functioning Hadoop environment
  
  ğŸ”¹Perform distributed data processing using MapReduce
  
  ğŸ”¹Understand file storage and replication in HDFS
  
  ğŸ”¹Evaluate performance parameters affecting job execution
  
  ğŸ”¹Use PySpark for large-scale text analysis
  
  ğŸ”¹Extract structured information from unstructured data
  
  ğŸ”¹Compute document similarity using TF-IDF

  ğŸ”¹Model relationships between authors based on publication data

ğŸ’»**Environment Details**

  ğŸ”¹Operating System: Ubuntu 22.04 LTS
  
  ğŸ”¹Hadoop: Version 3.3.x
  
  ğŸ”¹Apache Spark: Version 3.x
  
  ğŸ”¹Python: Version 3.x
  
  ğŸ”¹Java: OpenJDK

**Deployment Mode**: Single-node pseudo-distributed setup

âš™ï¸ Hadoop Implementation (Q1â€“Q9)

ğŸ”¹ MapReduce WordCount

The WordCount example program was executed to demonstrate distributed processing. The job divides input data into splits, processes them in parallel using mapper tasks, and aggregates results using reducers.

ğŸ”¹ HDFS Operations

Key file system operations performed include:

  ğŸ”¹Uploading files to HDFS
  
  ğŸ”¹Listing directory contents
  
  ğŸ”¹Understanding block storage
  
  ğŸ”¹Observing replication behavior
  
  ğŸ”¹Performance Evaluation

Execution time was measured to study how configuration parameters (such as input split size) influence performance.

âš¡ Spark-Based Analysis (Q10â€“Q12)

ğŸ”¹ Metadata Extraction

Metadata such as title, release date, language, and encoding was extracted from book headers using pattern matching techniques.

ğŸ”¹ Document Similarity Analysis

A similarity model was implemented using TF-IDF and cosine similarity. The workflow included:

  ğŸ”¹Cleaning and preprocessing text
  
  ğŸ”¹Tokenization
  
  ğŸ”¹Removing stop words
  
  ğŸ”¹Generating TF-IDF vectors
  
  ğŸ”¹Measuring similarity between documents
  
  ğŸ”¹Author Relationship Network

An approximate influence network was constructed by connecting authors whose works fall within a defined time window. This provides insights into potential literary relationships.

ğŸ§  **Concepts Applied**

  ğŸ”¹Distributed file storage (HDFS)
  
  ğŸ”¹Parallel computation with MapReduce
  
  ğŸ”¹Resource management
  
  ğŸ”¹Text mining techniques
  
  ğŸ”¹Regular expressions
  
  ğŸ”¹Vector space representation
  
  ğŸ”¹Similarity measurement
  
  ğŸ”¹Graph modeling using Spark

ğŸ“Š **Key Observations**

ğŸ”¹Hadoop efficiently handles large datasets using distributed processing.

ğŸ”¹System performance depends on configuration and input characteristics.

ğŸ”¹Spark enables faster analysis due to in-memory computation.

ğŸ”¹TF-IDF highlights distinctive words while reducing common terms.

ğŸ”¹Network modeling provides useful structural insights but remains approximate.

âš ï¸**Constraints and Assumptions**

ğŸ”¹Influence relationships are inferred, not explicitly verified.

ğŸ”¹Metadata quality varies across documents.

ğŸ”¹Cross-join operations may become expensive for very large datasets.
